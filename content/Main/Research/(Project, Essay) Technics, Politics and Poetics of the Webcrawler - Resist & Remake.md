[[webcrawler DevLog]]
[[AI Apaec]]
[[Science & Technology Studies]]

# The Web Crawler

Webcrawlers, also referred to as web spiders or web robots, are foundational to the mechanisms of modern search engines, playing a crucial role in harvesting data from the internet. These tools made their debut in the digital landscape in the early 1990s, with the World Wide Web Worm, developed by Oliver McBryan in March 1994, standing as a notable early example. This pioneering webcrawler endeavored to systematically catalog the web by navigating from link to link, thus mapping the internet in a way that made it searchable and accessible.

For those less familiar with technical jargon, it's useful to conceptualize webcrawlers as diligent explorers of the internet. They methodically visit websites, scan their content, and record the links found within. This process compiles a comprehensive index, akin to a library catalog, allowing search engines like Google and Bing to efficiently locate and retrieve information in response to user queries. The effectiveness and comprehensiveness of webcrawlers are vital for ensuring that search results are both broad-ranging and pertinent, facilitating users in finding the information they need swiftly. These digital navigators are in constant operation, updating their indexes with new data, revising records of existing web pages, and eliminating outdated links.

## Big Data

The intersection of webcrawlers and big data is significant, as webcrawlers play a key role in the accumulation and management of large datasets. Big data encompasses the extensive volumes of information generated daily through various digital activities, including social media use, online transactions, and more. Webcrawlers stand at the forefront of collecting this vast array of data, which is subsequently processed and analyzed for a myriad of purposes, such as market research, trend analysis, and the enhancement of search engine algorithms. The insights derived from the data collected by webcrawlers are used by corporations and state entities for understanding people's behavior, preferences, and sociocultural as well as political trends on the internet. As such, the function of webcrawlers extends beyond search engine optimization to influence diverse areas where big data is crucial for strategic decision-making and development. 

Big data technology has great potential. Both for good and nefarious purposes. Manipulative targeted advertising or political propaganda and disinformation are just two of many examples of big data being used to control populations and mislead individuals.

### What even is data?

However, the key term here is *potential*. Data is an object that generates it's potential in a processual flow through the apparatus that can receive and read it. 
First, data has to be received at all. If no data is received, the data and by extension the data-sending agent might as well not tangibly exist from the perspective of the receiving agent, even though the data objectively exists somewhere out there.

Then, the received data must be readable by the receiving agent. That is, molded into a form that makes 'sense' to the receiving system.

Finally, received data must be analyzed. By combining actual data with past data of the same or a similar category for statistical predictions, for example. This last step is where the potential of data becomes actionable, both for good and bad. But the outcome of analysis also depends on the steps before.

Additionally, any complete analysis will acknowledge that data is nothing exclusive to the digital realm. Consider the breadth of data that is generated every second all around our physical bodies which we don't even realize. Organisms that are so small, they are imperceivable to our eyes, yet they are still there and could make one very sick. And were it not for the invention of the microscope - an apparatus to make microorganisms visible, thus readable and analyzable - we still would be reliant on our superstitious imaginations to protect ourselves from dangerous sicknesses.

To bring my point back to the webcrawler: It is first and foremost a device like the microscope. The webcrawler enables us to perceive data that may be there all along, but is not accessible to us any other way. Maybe because a webpage is not indexed at all and cannot be accessed otherwise. Or because it is just one among so many other webpages that our built-in data processing tools (our brains) are simply unable to keep all this information on it's own.
## **Purpose and Function**

At their core, webcrawlers serve the function of automating the collection of web page data. This automation is crucial for search engines, which rely on up-to-date and comprehensive indexes to provide users with relevant search results. By constantly scanning the web for new content, updates to existing pages, and removing links to pages that are no longer available, webcrawlers ensure that search engine indexes remain accurate and reflective of the current state of the web.

Webcrawlers start their journey from a set of seed URLs, which act as the starting points for their exploration. As they visit these web pages, they identify all the hyperlinks within each page and add them to their list of pages to visit, effectively creating a map of the internet's structure. This process, driven by algorithms that determine which pages to visit and when, allows webcrawlers to cover the vastness of the web systematically.

Critically, the analytical structuring of webpages follows the internal logic of the apparatus that conducts this structuring. In case of Google - or rather Alphabet - a profit-oriented corporation. Webpages are ranked first and foremost by how much they pay directly to google's ad system. But this simple capital for attention model is just the tip of the iceberg. 
## Search Engines

The vast majority of webhosts do not pay Alphabet anything, yet their content is still indexed and displayed by Google. The index ranking is determined by Google's internal and intransparent algorithmic ranking logic as well as Google's requirements for structuring metadata, which is also not always completely transparent and constantly changing. This culminates in webhosts having to do continuous labor for Google and other webcrawlers to structure and design their content in such a way as to gain a favorable ranking position. A whole industry called search engine optimization (SEO) has emerged around this phenomenon.

This indirect power over content creator's behavior is Alphabet's, and other big data companie's, real capital. Much more valuable than the money they receive for advertising. Even though webhosts may run their websites from their own servers, they are still dependant on Google's webcrawler indexing policy in order for them to be perceivable. 
This means that Google is for all practical purposes a platform just like any other social media site or digital distribution system. 
Even though an online shop may run on their own server in the store owner's basement, it's business and existence hinges on the page being indexed by a webcrawler and thus perceivable by potential customers. Were Google the only webcrawler out there, they could extract exorbitant rents on billions of businesses.  
## Platforms and API Practices

More and more, platforms are starting to aggressively capitalize on their data power. Recently, Reddit, a major player in the social media landscape, exemplifies this trend by refining its approach to Application Programming Interfaces (APIs) management. APIs, the conduits that allow disparate software systems to communicate and share data, are pivotal for platforms like Reddit in managing the vast ecosystem of third-party applications, bots, and integrations that contribute to the user experience and platform's growth.

Reddit's journey with APIs highlights the broader shift in platform strategies from open access to more regulated and monetized models. Initially, Reddit's APIs were relatively open, encouraging the development of a wide array of third-party apps, tools, and services that drove engagement and innovation on the platform. However, as the value of data and the importance of controlling how that data is accessed and used became more apparent, Reddit, like many platforms, began to implement more stringent API policies, making actors who want to analyze and repurpose reddit's user-generated content, pay a lot of money for that data. Money, that many institutions and people are unable to afford.

Reddit's example is one among many dangers that platform data monopolism brings as those with power over data may develop a technological ecosystem that relies on said data, only to one day aggressively monetize said data while the users who's business relies on this data, have no choice but to pay.

## AI and the future of Webcrawling

The future of webcrawling is intricately tied to the advancements in artificial intelligence (AI), heralding a new era where webcrawlers become far more sophisticated, and efficient in navigating the vast and ever-expanding digital universe. As AI technologies evolve, we can anticipate webcrawlers that are not only capable of understanding and indexing web content with unprecedented depth and nuance but also able to discern the context, sentiment, and relevance of information in ways that mimic human cognitive processes. This leap forward will significantly enhance search engines' ability to deliver highly relevant and personalized search results, transforming how users interact with the internet. Furthermore, AI-driven webcrawlers could autonomously adapt to changes in web technologies and user behaviors, ensuring that the digital ecosystem remains accessible and navigable despite its continuous evolution. By leveraging machine learning algorithms, natural language processing, and other AI methodologies, the next generation of webcrawlers will unlock new potentials in data analysis, SEO strategies, and online content discovery, fundamentally shifting the landscape of digital information retrieval and utilization.

## Automating Everything: Large Action Models

The integration of Artificial Intelligence (AI) and Large Action Models (LAMs) in webcrawling signifies a groundbreaking next step in the rapidly evolving automation landscape.

**Large Action Models** are an AI system originally developed by [Rabbit research team](https://www.rabbit.tech/research). Its goal is to not only automate the indexing and data-extraction of websites, but also the deployment of autonomous agents that can navigate any website or user-interface just like a human would while retaining the ability to store and process large amounts of data like a machine.
This (theoretically) allows the Large Action Model to navigate any website, whether it is optimized for the particular webcrawler or not. And it also allows it to perform any action on this website. Whether that is to find, extract and restructure large amounts of data or to perform actions such as booking a ticket or playing a video.

## From Prosthetic Gods to Digital Collectives

Freud once described "man" as "prosthetic gods," leveraging technology to extend their capabilities beyond natural limits. In today's digital age, this concept has evolved dramatically. Society, or more specifically, 'the man,' has crafted a network of interconnected, autonomously functioning, hive-minded spider-bots. These digital entities roam the vast expanse of the internet, harvesting information to feed their systems, then reshaping and deploying it in ways that serve their creators' interests to ultimately return to conquer and reshape the internet with the information they have gathered. A digital continuity of man's age-old colonial practices.

How do we respond to this expansive control exerted by 'the man'? The solution lies in democratizing this power, in returning it to the hands of the people. It involves empowering individuals and communities alike to create and deploy their own versions of these prosthetic, hive-minded spider-bots. This approach aims to decentralize the control of information and thus the process of meaning-making, enabling people to reclaim their influence over data and its significance in our lives.

# Resist & Remake

**In order to establish countermeasures against the growing power of platforms, it is neccesary for people to take back the power over the means of data production.**

We have largely fallen for the outright lie that technology is arcane, forbidden knowledge only a few individuals blessed by the gods can ever know.
As ridiculous as that sounds. In my experience, it rings absolutely true. Ordinary people tend to treat technology with distanced fear and defeated apathy. Having accepted that the technology they are using daily and which they have come to rely on is not their technology, that is, prosthetics of their own body. But rather extensions of a foreign body that is controlling *them*.

At most, people resist this foreign invasion by shedding technology altogether or at least partially. By minimizing their reliance on technology. Yet the world keeps on moving and technology keeps on developing. 
As states are engulfed in a renewed arms race - this time centered around AI - I fear that no amount of reason or popular concern will stop or even slow the rapid advancement and all-encompassing adoption of digital technology. Ordinary people may get left behind and further disenfranchised in this process.

But I also blame educators and old-guard technologists. 
They love to keep people in the dark as much as corporate entities. More for reasons of ego than profit. Why is it that every introduction to programming is so unneccesarily abstract. Sure, a fledgling computer scientist should know about the very basics of data structures, system architecture and the mathematics of low-level programming.
But this has as much to do with the practical day-to-day application of digital worlds as molecular structures or the laws of physics has to do with the day-to-day physical world.

In both cases, even through years of complaint, education has not shown any interest in developing a curriculum that teaches people to navigate socioculturally established practices and structures. This must also extend to navigating established structures of digital technologies such as operating systems, the web or AI. 

And do not even dare, as a beginner and non-computer scientist, to ask a question about programming to an old-guard technologist on a forum like Reddit or Stack Overflow.
You will only be met with disgruntled answers, arrogant dismissal and flippant downvotes. Old-guard technologists have, over the years, fostered a tight-knit, very homogenous community. And they don't seem to have any intention of opening up. We need diversity in tech? Start right there.

There was scarcely any choice for people who want to get into tech and who do not have the means to complete a full computer science degree or are part of an exclusive online tech community, to take up some of the big-tech sponsored microdegrees that are tailored to the needs of the Sillicon Valley human resource demands.

However,
The advent of generative AI brings an alternative. Generative AI, such as ChatGPT, doesn't only generate flowery poems and anatomically incorrect portraits of imaginary humans.
It can also generate whole curricula tailored to an individual's specific learning style, needs, goals and previous knowledge.
Combined with the webcrawler, it can even sort and compile the latest new developments into individually tailored actionable knowledge.
And it can answer any question you might have at your own pace at any time.

But do not be fooled. This is not the purpose the owners of ChatGPT and similar alternatives have in mind for the technology. They do not, under any cirumstances, want people to know how to reprogram their services or develop alternatives. They only tolerate giving out this knowledge as far as they still need labor to develop their products. Unpaid or paid.
The goal big tech has in mind for AI is transparently stated. By the likes of Sam Altman (CEO of OpenAI) 'Artificial general intelligence' - autonomous artificial agents that do whatever the user desires for them. If it is permitted, of course.

As the world becomes more and more reliant on digital technology, Big Tech somehow has *less* demand for people to know about technology. Recently, Jensen Huang, CEO of Nvidia, has said that Kids should not learn to code anymore. As their product, autonomous AI agents, will do that job for them. [Why Nvidia's CEO Thinks Kids Shouldn't Learn to Code Anymore (techthelead.com)](https://techthelead.com/why-nvidias-ceo-thinks-kids-shouldnt-learn-to-code-anymore/#:~:text=Nvidia%20CEO%20Jensen%20Huang%E2%80%99s%20latest%20statement%20has%20made,to%20learn%20programming%2C%20as%20AI%20can%20do%20that.)

Do not be fooled by this. As Google is the platformization of the web - autonomous AI agents controlled by big tech will be the platformization of computer technology as a whole. Obfuscated by their profit-driven needs.

Keep learning. Use AI - corporate or open source, whatever works for you. And drop it if it doesn't work anymore or there is a better alternative. Think in terms of utility, not in terms of branding, lifestyle or aesthetics. Be a creator, not a 'user'.

# Making our own Webcrawler using generative AI

One example how we can take back control over technology is by making our own tools. Specifically for the collection of big data for further use.

Making a webcrawler for most specific websites for a specific purpose is not hard. Especially generative AI such as ChatGPT is very helpful in quickly creating the neccesary lines of code.

However, setting up the environment and all the other neccesary steps can be a challenge especially for non-technical users. I will use this space to further refine documentation, write step-by-step tutorials and give further resources. But my time is limited. If there are any questions or you want to get in touch, I have set up a discord server: https://discord.gg/GqF3fNk4
You can also contact me at Jay.Rathjen55@gmail.com
I might answer an email late though.

Now onto the explanation.

``` Python
import os
from selenium import webdriver
import pandas as pd

driver = webdriver.Chrome()
# Initialize the Chrome WebDriver
# Open the webpage
driver.get("https://schwarzesbrett.bremen.de/verkauf-angebote/rubrik/arbeitsplatzangebote-verkauf.html")
elements = driver.find_elements("xpath", '//*[@id="eintraege"]//a')
data = []

    # Iterate over the rows in pairs (job title, date, URL)
for element in elements:
        job_title = element.text.split('\n')[0]
        date = element.text.split('\n')[1] if '\n' in element.text else None
        url = element.get_attribute('href')
        data.append([job_title, date, url])
# Create a DataFrame from the data
df = pd.DataFrame(data, columns=['Job Title', 'Date', 'URL'])
print(df)
# Optional define file path for output
#file_path = os.path.join('Data', 'Crawlers', 'CrawlerOutput', 'SchwBrtt_HB.csv')
#os.makedirs(os.path.dirname(file_path), exist_ok=True)
#df.to_csv(file_path, index=False)
df.to_csv(index=False)
#print(f"Data saved to {file_path}")
```

This code is a simple webcrawler for the site https://schwarzesbrett.bremen.de/verkauf-angebote/rubrik/arbeitsplatzangebote-verkauf.html

This is what this code does:
1. It imports the neccesary libraries, that is programs within a program. For example, Selenium is a framework for automating actions in a browser. In this case, Selenium finds a pre-defined place on the website and extracts all the information from this page.
2. Initialize Webdriver. That is another third party program that connects to a Chrome installation to automate the browser.
3. The 'xpath' in elements is the specific location of the website where information should be extracted. More on this later.
4. data = [] is an instruction to create a table for pandas. Pandas is a library for data manipulation.
5. for element in elements and the lines under are the **parser** bit. This reads the raw HTML data that has been extracted from the xpath and restructures it so that it fits neatly into the predefined categories for our table.
6. Then the data is displayed as a table in the terminal and saved to a csv file.

I will now reconstruct the steps used to build the code above so that you can do it yourself for your own use-case.

### 1. Identify the information to crawl

Go to the desired webpage. In this case: [Angebote für Jobs in Bremen und umzu - Schwarzes Brett](https://schwarzesbrett.bremen.de/verkauf-angebote/rubrik/arbeitsplatzangebote-verkauf.html)
Then press F12 to open the developer tools. This should be the same in any browser, I am using Brave in this example.

![[Pasted image 20240307181609.png]]

Click 'Select an element in the page to inspect it' then hover over the page until you identify the element that you would like to extract.

![[Pasted image 20240307181757.png]]

In this case, we want to extract the list of jobs on this page. As we hover over it, the element will be highlighted and the selector in the list jumps to the right element. Right click on the element under copy and select 'copy xpath'.
This will hopefully copy the right identifier. In our case it's this: //*[@id="eintraege"]


## Using ChatGPT to code a webcrawler for a specific purpose

I am using ChatGPT as an example throughout this post. However, any other generative AI chat could be used. A good example of an alternative is Mistral. The french AI company is in many ways even better than ChatGPT.

Now, it is important to realize that in order for generative AI to output the right desired content, it needs **clearly communicated directions** and, above all, **Context**.

In our case that means we have to spell out what we want and we need to provide information about how we want it.

```
I want to create a webcrawler using selenium and pandas. Below is an example code. You will make a new crawler with new requirements strictly based on the example code. (example code) import os from selenium import webdriver import pandas as pd driver = webdriver.Chrome() # Initialize the Chrome WebDriver # Open the webpage driver.get("https://schwarzesbrett.bremen.de/verkauf-angebote/rubrik/arbeitsplatzangebote-verkauf.html") elements = driver.find_elements("xpath", '//*[@id="eintraege"]//a') data = [] # Iterate over the rows in pairs (job title, date, URL) for element in elements: job_title = element.text.split('\n')[0] date = element.text.split('\n')[1] if '\n' in element.text else None url = element.get_attribute('href') data.append([job_title, date, url]) # Create a DataFrame from the data df = pd.DataFrame(data, columns=['Job Title', 'Date', 'URL']) print(df) # Optional define file path for output #file_path = os.path.join('Data', 'Crawlers', 'CrawlerOutput', 'SchwBrtt_HB.csv') #os.makedirs(os.path.dirname(file_path), exist_ok=True) #df.to_csv(file_path, index=False) df.to_csv(index=False) #print(f"Data saved to {file_path}") (New Crawler) URL: https://schwarzesbrett.bremen.de/verkauf-angebote/rubrik/arbeitsplatzangebote-verkauf.html xpath: //*[@id="eintraege"] (Instructions) I want to extract all the job listings in the mentioned xpath. For now, extract the html of the mentioned xpath so that i can show you the html structure later.
```

This is the prompt i am using. ChatGPT Premium or other AI services have features that simplify this process by being able to define the context instructions outside of the prompt or fine-tuning the model by uploading information such as this code into a data repository. But essentially, a prompt with enough information should be all you need.

This prompt will create code to extract the raw html first. In my case, ChatGPT output this code:

``` Python
from selenium import webdriver

# Initialize the Chrome WebDriver
driver = webdriver.Chrome()

# Open the webpage
driver.get("https://schwarzesbrett.bremen.de/verkauf-angebote/rubrik/arbeitsplatzangebote-verkauf.html")

# Find the element by XPath
element = driver.find_element("xpath", '//*[@id="eintraege"]')

# Extract the HTML of the element
element_html = element.get_attribute('outerHTML')

# Close the browser
driver.quit()

# Print the HTML content
print(element_html)

# Optionally, you can save this HTML content to a file for further analysis
with open('element_html.txt', 'w', encoding='utf-8') as file:
    file.write(element_html)
    print("HTML content saved to element_html.txt")

```

If we can successfully run this code, it will output the raw html. Now, we need to refine the crawler to sort the raw html into a neat dataframe structure.
I ask chatGPT again with the following prompt including a snippet of the output from the code before

```
Now, refine this crawler to parse the information into a dataframe. Below is an example HTML snippet. Use this as a reference to create a parser to turn all the job data into a dataframe for DYNAMIC extraction of the data: </li></ul><div class="separator_ads"> <div class="teaser_wrapper"> <span class="ad_head">Zum Thema auf bremen.de: </span> <div class="teaser"> <div class="teaserbild"> <img alt="Laternenumzug am Abend: Zahlreiche Menschen stehen mit Laternen auf dem Bremer Marktplatz vor dem Rathaus. " title="Laternenumzug am Abend: Zahlreiche Menschen stehen mit Laternen auf dem Bremer Marktplatz vor dem Rathaus. " style="width:100px" src="https://medien.bremen.de/fastmedia/34/thumbnails/bremer-lichtermeer-jazz-promotion.jpg.347911.jpg"> </div> <div class="teasercontent"> <h2><a href="https://www.bremen.de/leben-in-bremen/bremen-vier-lichtermeer" target="_blank">Bremen Vier Lichtermeer</a></h2> <p>Der größte Laternenumzug in Bremen. <a class="more" href="https://www.bremen.de/leben-in-bremen/bremen-vier-lichtermeer" title="Bremen Vier Lichtermeer" target="_blank">Mehr</a> </p> </div> <hr style="clear:both;visibility:hidden"> </div> </div> </div> <span class="date_headline" id="24_02_15"> Donnerstag, 15.02.24 </span><ul class="content_list eintraege_list"> <li class=" first gewerblich"> <a href="/verkauf-angebote/arbeitsplatzangebote-verkauf/countermitarbeiter-mwd-23140465.html"> Countermitarbeiter (m/w/d) <span class="list_date"> 15.02.24 </span> <span class="fa fa-eur"></span> <span class="fa fa-angle-right"></span> </a> </li> <li class=" gewerblich"> <a href="/verkauf-angebote/arbeitsplatzangebote-verkauf/servicemitarbeiter-mwd-23140462.html"> Servicemitarbeiter (m/w/d) <span class="list_date"> 15.02.24 </span> <span class="fa fa-eur"></span> <span class="fa fa-angle-right"></span> </a> </li> <li class=" gewerblich"> <a href="/verkauf-angebote/arbeitsplatzangebote-verkauf/barmitarbeiter-mwd-23140453.html"> Barmitarbeiter (m/w/d) <span class="list_date"> 15.02.24 </span> <span class="fa fa-eur"></span>
```

This results in the GPT outputting the following code:

```python
from selenium import webdriver
from bs4 import BeautifulSoup
import pandas as pd

# Initialize the Chrome WebDriver
driver = webdriver.Chrome()

# Open the webpage
driver.get("https://schwarzesbrett.bremen.de/verkauf-angebote/rubrik/arbeitsplatzangebote-verkauf.html")

# Find the element by XPath that contains job listings
element = driver.find_element("xpath", '//*[@id="eintraege"]')

# Extract the HTML of the element
element_html = element.get_attribute('outerHTML')

# Close the browser
driver.quit()

# Parse the extracted HTML with BeautifulSoup
soup = BeautifulSoup(element_html, 'html.parser')

# Initialize an empty list to store job data
jobs_data = []

# Extract job listings. Assuming each job listing is contained within 'li' tags
# with class 'gewerblich' within the extracted 'element_html'
job_listings = soup.find_all('li', class_='gewerblich')

# Loop through each job listing to extract information
for job in job_listings:
    job_link_tag = job.find('a')
    if job_link_tag:
        job_title = job_link_tag.get_text(strip=True)
        job_url = job_link_tag['href']
        job_date = job.find('span', class_='list_date').get_text(strip=True)
        
        # Append the job data to the list
        jobs_data.append({
            'Job Title': job_title,
            'URL': job_url,
            'Date': job_date
        })

# Convert the list of dictionaries to a DataFrame
jobs_df = pd.DataFrame(jobs_data)

# Optionally, you can save this HTML content to a file for further analysis
with open('element_html.txt', 'w', encoding='utf-8') as file:
    file.write(element_html)
    print("HTML content saved to element_html.txt")

# Display the DataFrame to verify the extracted data
print(jobs_df)

```

Using the right **Prompting strategy** you can create a webcrawler for pretty much any purpose. With more developments in Large Action Model development, it's likely also going to be possible to fully automate this process so that you only need to input your purpose and the action model will do the rest.

However, as alluded to above. It is prudent that we understand what is going on 'under the hood'. So that we don't lose track of where the data is coming from that we want to extract and how exactly we are manipulating this data. The method i have demonstrated is both feasible for non-technical users, economically quick and also 100% transparent so that it is safe from outside manipulation and obfuscation.